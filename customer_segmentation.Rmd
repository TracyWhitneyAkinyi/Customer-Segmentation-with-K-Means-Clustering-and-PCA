---
title: "Customer Segmentation using K-Means Clustering and PCA"
author: "Tracy Whitney Akinyi"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

In this project we are going to perform one of the applications of unsupervised machine learning - customer segmentation.In this project, we will implement customer segmentation in R.

In this project we will explore the data by performing descriptive analysis, exploratory analysis and implement different versions of K-means algorithm.
Customer segmentation is the process by which you divide your customers into segments up based on common characteristics – such as demographics or behaviors, so you can market to those customers more effectively.Using clustering techniques, companies can identify the several segments of customers allowing them to target the potential user base. In this machine learning project, we will make use of K-means clustering which is the essential algorithm for clustering unlabeled data set.

Companies that deploy customer segmentation are under the notion that every customer has different requirements and require a specific marketing effort to address them appropriately.

# *Data Exploration*

### *_Load libraries_*
```{r}
library(readr)
library(dlookr)
library(cluster)
library(grid)
library(gridExtra)
library(NbClust)
library(factoextra)
library(GGally)
library(flextable)
library(ggstatsplot)
library(tidyverse)
```

### _Load data set_
```{r}
cs <- read_csv("D:/R Directories/Customer Segmentation/data/Mall_Customers.csv")
View(cs)
```

### _Descriptive summaries_
```{r}
dlookr::describe(cs,quantiles = c(.25,.50,.75)) %>% flextable()
```

```{r}
DF = cs[,c("Age","Annual Income (k$)","Spending Score (1-100)")]
ggpairs(DF)
```

## _Rename Genre as Gender_
```{r}
cs <- rename(cs,Gender = Genre)
```

## _Gender distribution_
```{r}
cs %>% ggplot(aes(x=Gender,fill = Gender))+
       geom_bar()+
     ggtitle("Customer Gender comparison")+
      geom_text(aes(label = ..count..),stat = "count", vjust = 1.5, colour = "white")
```

From the above bar plot we can observe that the number of females are more than males.

## _Distribution of Annual Income_
```{r}
cs %>% ggplot(aes(x= `Annual Income (k$)`, color = Gender)) +
  geom_histogram(fill = "grey", position="dodge")


```

## _Distribution of Spending Score_
```{r}
cs %>% ggplot(aes(x= `Spending Score (1-100)`, color = Gender)) +
  geom_histogram(fill = "white", position="dodge")
```
## _Distribution of Age_
```{r}
cs %>% ggplot(aes(x= Age, color = Gender)) +
  geom_histogram(fill = "white", position="dodge")
```

## _Comparing Gender with Annual Income_
```{r}
cs %>% ggbetweenstats(x=Gender,y=`Annual Income (k$)`,type = "np")
```

## _Comparing Gender with Spending Score_
```{r}
cs %>% ggbetweenstats(x=Gender,y=`Spending Score (1-100)`,type = "np")
```
The Annual Income and Spending Score between the genders doesn't differ significantly.

# *K - means Algorithm*

While using the k-means clustering algorithm, the first step is to indicate the number of clusters (k) that we wish to produce in the final output. The algorithm starts by selecting k objects from data set randomly that will serve as the initial centers for our clusters.
This link talks more about K- means clustering in R:

 https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/ 

While working with clusters, you need to specify the number of clusters to use. You would like to utilize the optimal number of clusters. To help you in determining the optimal clusters, there are two popular methods –

    *Elbow method
    *Silhouette method

## _Elbow method_
```{r}

set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(cs[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}

k.values <- 1:10


iss_values <- map_dbl(k.values, iss)

plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")
```
From the above graph, we conclude that 4 appears to be the appropriate number of clusters since it seems to be appearing at the bend in the elbow plot. Let's try the silhouette method. 


### _Silhouette method_
```{r}
fviz_nbclust(cs[,3:5], kmeans, method = "silhouette")
```

From the above graph, we conclude that 6 appears is the appropriate number of clusters

### Computing gap statistic
```{r}
k6<-kmeans(cs[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6
```

### _Calculating Principal Component Analysis_

```{r}
pca=prcomp(cs[,3:5],scale=FALSE) #principal component analysis
summary(pca)

pca$rotation[,1:2]
```

### _Visualizing the clusters_
```{r}
set.seed(1)
cs %>% ggplot(aes(x =`Annual Income (k$)`, y = `Spending Score (1-100)`)) + 
      geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
    scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

 * Cluster 1 and 5 represent customers with medium average income and spending score.
 
 * Cluster 2 represents customers with low annual income ans low spending score.
 
 * Cluster 3 represents customers with high annual income and low spending score.
 
 * Cluster 4 represents customers with high annual income and high spending score.
 
 * Cluster 6 represents customers with low annual income and high spending score.
 
 


```{r}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters

plot(pca$x[,1:2], col =kCols(digCluster),pch =19,xlab ="PCA 1",ylab="PCA 2")
legend("topright",unique(dignm),fill=unique(kCols(digCluster)))
```

* Cluster 1 and 5 have medium PCA 1 and PCA 2
* Cluster 2 has a high PCA 1 and medium PCA 2
* Cluster 3 has a medium PCA 1 and low PCA 2
* Cluster 4 has a low PCA 1 and medium PCA 2
* Cluster 6 has a medium PCA 1 and high PCA 2.


